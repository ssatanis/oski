"""
Complete Document Analysis System
Handles PDF, DOC, DOCX, XLS, XLSX, TXT, CSV, and images with AI integration
Integrates OCR, AWS Textract, and Google Vision for comprehensive analysis
"""

import os
import json
import logging
import base64
import io
from typing import Dict, List, Any, Optional, Union
from pathlib import Path
import mimetypes

# Core document processing libraries
import pandas as pd
import openpyxl
from openpyxl.utils.dataframe import dataframe_to_rows
import docx
from docx.document import Document
import pytesseract
from PIL import Image
import fitz  # PyMuPDF for PDFs
import xlrd
import csv
import chardet

# AWS Textract
import boto3
from botocore.exceptions import NoCredentialsError, ClientError

# Google Vision
from google.cloud import vision
from google.oauth2 import service_account

# Image processing
import cv2
import numpy as np

# Advanced text processing
import re
from collections import defaultdict

class ComprehensiveDocumentAnalyzer:
    """
    Production-ready document analyzer that integrates multiple OCR services
    and provides thorough analysis of medical rubrics and assessment documents.
    """
    
    def __init__(self, aws_access_key=None, aws_secret_key=None, aws_region='us-east-1', 
                 google_credentials_path=None):
        """
        Initialize the document analyzer with multiple OCR providers
        
        Args:
            aws_access_key: AWS access key for Textract
            aws_secret_key: AWS secret key for Textract
            aws_region: AWS region for Textract
            google_credentials_path: Path to Google Cloud service account JSON
        """
        self.setup_logging()
        self.setup_aws_textract(aws_access_key, aws_secret_key, aws_region)
        self.setup_google_vision(google_credentials_path)
        self.setup_tesseract()
        
        # Supported formats with their processors
        self.processors = {
            '.pdf': self.process_pdf,
            '.doc': self.process_doc,
            '.docx': self.process_docx,
            '.xls': self.process_xls,
            '.xlsx': self.process_xlsx,
            '.txt': self.process_txt,
            '.csv': self.process_csv,
            '.png': self.process_image,
            '.jpg': self.process_image,
            '.jpeg': self.process_image,
            '.tiff': self.process_image,
            '.bmp': self.process_image,
            '.gif': self.process_image
        }
    
    def setup_logging(self):
        """Setup comprehensive logging"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        self.logger = logging.getLogger(__name__)
    
    def setup_aws_textract(self, access_key, secret_key, region):
        """Setup AWS Textract client"""
        try:
            if access_key and secret_key:
                self.textract_client = boto3.client(
                    'textract',
                    aws_access_key_id=access_key,
                    aws_secret_access_key=secret_key,
                    region_name=region
                )
            else:
                # Use default credentials (IAM role, environment variables, etc.)
                self.textract_client = boto3.client('textract', region_name=region)
            
            self.logger.info("AWS Textract client initialized successfully")
            self.textract_available = True
        except Exception as e:
            self.logger.error(f"Failed to setup AWS Textract: {e}")
            self.textract_client = None
            self.textract_available = False
    
    def setup_google_vision(self, credentials_path):
        """Setup Google Cloud Vision client"""
        try:
            if credentials_path and os.path.exists(credentials_path):
                credentials = service_account.Credentials.from_service_account_file(credentials_path)
                self.vision_client = vision.ImageAnnotatorClient(credentials=credentials)
            else:
                # Use default credentials
                self.vision_client = vision.ImageAnnotatorClient()
            
            self.logger.info("Google Cloud Vision client initialized successfully")
            self.vision_available = True
        except Exception as e:
            self.logger.error(f"Failed to setup Google Cloud Vision: {e}")
            self.vision_client = None
            self.vision_available = False
    
    def setup_tesseract(self):
        """Setup Tesseract OCR"""
        try:
            # Test Tesseract availability
            pytesseract.get_tesseract_version()
            self.tesseract_available = True
            self.logger.info("Tesseract OCR initialized successfully")
        except Exception as e:
            self.logger.error(f"Tesseract not available: {e}")
            self.tesseract_available = False

    def analyze_multiple_documents(self, file_paths: List[str]) -> Dict[str, Any]:
        """
        Analyze multiple documents and combine results
        
        Args:
            file_paths: List of file paths to analyze
            
        Returns:
            Combined analysis results
        """
        self.logger.info(f"Analyzing {len(file_paths)} documents")
        
        combined_results = {
            "documents": [],
            "combined_analysis": {
                "total_files": len(file_paths),
                "successful_extractions": 0,
                "failed_extractions": 0,
                "combined_text": "",
                "merged_criteria": [],
                "dominant_structure": None
            }
        }
        
        all_texts = []
        all_criteria = []
        structure_types = []
        
        for file_path in file_paths:
            try:
                result = self.analyze_document(file_path)
                combined_results["documents"].append(result)
                
                if result["processing_status"] == "success":
                    combined_results["combined_analysis"]["successful_extractions"] += 1
                    
                    # Collect text
                    text_content = result["extraction"].get("text_content", "")
                    if text_content.strip():
                        all_texts.append(text_content)
                    
                    # Collect criteria
                    if "rubric_analysis" in result and "criteria" in result["rubric_analysis"]:
                        all_criteria.extend(result["rubric_analysis"]["criteria"])
                    
                    # Collect structure types
                    if "structure" in result and "document_type" in result["structure"]:
                        structure_types.append(result["structure"]["document_type"])
                        
                else:
                    combined_results["combined_analysis"]["failed_extractions"] += 1
                    
            except Exception as e:
                self.logger.error(f"Failed to analyze {file_path}: {e}")
                combined_results["combined_analysis"]["failed_extractions"] += 1
        
        # Combine and analyze results
        combined_results["combined_analysis"]["combined_text"] = "\n\n".join(all_texts)
        combined_results["combined_analysis"]["merged_criteria"] = self.merge_criteria(all_criteria)
        combined_results["combined_analysis"]["dominant_structure"] = self.determine_dominant_structure(structure_types)
        
        # Perform cross-document analysis
        if len(all_texts) > 1:
            combined_results["combined_analysis"]["cross_document_insights"] = self.analyze_cross_document_patterns(all_texts)
        
        return combined_results

    def analyze_document(self, file_path: str) -> Dict[str, Any]:
        """
        Main entry point: Analyze any supported document format with comprehensive OCR
        
        Returns comprehensive analysis including:
        - Multi-OCR content extraction (Tesseract + AWS Textract + Google Vision)
        - Structural analysis
        - Medical rubric-specific insights
        - Metadata and statistics
        """
        try:
            file_path = Path(file_path)
            if not file_path.exists():
                raise FileNotFoundError(f"File not found: {file_path}")
            
            # Detect file type
            file_extension = file_path.suffix.lower()
            if file_extension not in self.processors:
                raise ValueError(f"Unsupported file format: {file_extension}")
            
            self.logger.info(f"Processing {file_extension} file: {file_path.name}")
            
            # Extract content using appropriate processor with multi-OCR
            extraction_result = self.processors[file_extension](file_path)
            
            # Perform structural analysis
            structural_analysis = self.analyze_structure(extraction_result)
            
            # Perform medical rubric-specific analysis
            rubric_analysis = self.analyze_medical_rubric(extraction_result, structural_analysis)
            
            # Compile final analysis
            analysis = {
                "file_info": {
                    "name": file_path.name,
                    "size": file_path.stat().st_size,
                    "format": file_extension,
                    "mime_type": mimetypes.guess_type(str(file_path))[0]
                },
                "extraction": extraction_result,
                "structure": structural_analysis,
                "rubric_analysis": rubric_analysis,
                "processing_status": "success"
            }
            
            return analysis
            
        except Exception as e:
            self.logger.error(f"Document analysis failed: {e}")
            return {
                "file_info": {"name": str(file_path), "error": str(e)},
                "processing_status": "failed",
                "error": str(e)
            }
    
    # =============== MULTI-OCR IMAGE PROCESSING ===============
    
    def process_image_with_multi_ocr(self, image_path: Path) -> Dict[str, Any]:
        """
        Process image with multiple OCR engines for maximum accuracy
        """
        self.logger.info(f"Processing image with multi-OCR: {image_path.name}")
        
        ocr_results = {
            "tesseract": None,
            "aws_textract": None,
            "google_vision": None,
            "combined_text": "",
            "confidence_scores": {},
            "best_result": None
        }
        
        # Read image
        try:
            image = Image.open(image_path)
            image_bytes = self.image_to_bytes(image)
        except Exception as e:
            self.logger.error(f"Failed to read image: {e}")
            return ocr_results
        
        # Process with Tesseract
        if self.tesseract_available:
            try:
                tesseract_result = self.process_with_tesseract(image)
                ocr_results["tesseract"] = tesseract_result
                self.logger.info("Tesseract OCR completed")
            except Exception as e:
                self.logger.error(f"Tesseract OCR failed: {e}")
        
        # Process with AWS Textract
        if self.textract_available:
            try:
                textract_result = self.process_with_textract(image_bytes)
                ocr_results["aws_textract"] = textract_result
                self.logger.info("AWS Textract completed")
            except Exception as e:
                self.logger.error(f"AWS Textract failed: {e}")
        
        # Process with Google Vision
        if self.vision_available:
            try:
                vision_result = self.process_with_google_vision(image_bytes)
                ocr_results["google_vision"] = vision_result
                self.logger.info("Google Vision completed")
            except Exception as e:
                self.logger.error(f"Google Vision failed: {e}")
        
        # Combine and determine best result
        ocr_results["combined_text"] = self.combine_ocr_results(ocr_results)
        ocr_results["best_result"] = self.determine_best_ocr_result(ocr_results)
        
        return ocr_results
    
    def process_with_tesseract(self, image: Image.Image) -> Dict[str, Any]:
        """Process image with Tesseract OCR"""
        text = pytesseract.image_to_string(image, config='--psm 6')
        data = pytesseract.image_to_data(image, output_type=pytesseract.Output.DICT)
        
        # Calculate confidence
        confidences = [int(conf) for conf in data['conf'] if int(conf) > 0]
        avg_confidence = sum(confidences) / len(confidences) if confidences else 0
        
        return {
            "text": text,
            "confidence": avg_confidence,
            "word_count": len([word for word in data['text'] if word.strip()]),
            "detailed_data": data
        }
    
    def process_with_textract(self, image_bytes: bytes) -> Dict[str, Any]:
        """Process image with AWS Textract"""
        try:
            response = self.textract_client.detect_document_text(
                Document={'Bytes': image_bytes}
            )
            
            # Extract text and confidence
            text_parts = []
            confidences = []
            
            for item in response['Blocks']:
                if item['BlockType'] == 'LINE':
                    text_parts.append(item['Text'])
                    confidences.append(item['Confidence'])
            
            text = '\n'.join(text_parts)
            avg_confidence = sum(confidences) / len(confidences) if confidences else 0
            
            return {
                "text": text,
                "confidence": avg_confidence,
                "blocks": response['Blocks'],
                "line_count": len([b for b in response['Blocks'] if b['BlockType'] == 'LINE'])
            }
            
        except Exception as e:
            self.logger.error(f"AWS Textract processing failed: {e}")
            return {"text": "", "confidence": 0, "error": str(e)}
    
    def process_with_google_vision(self, image_bytes: bytes) -> Dict[str, Any]:
        """Process image with Google Cloud Vision"""
        try:
            image = vision.Image(content=image_bytes)
            response = self.vision_client.text_detection(image=image)
            
            if response.error.message:
                raise Exception(f"Google Vision API error: {response.error.message}")
            
            texts = response.text_annotations
            if not texts:
                return {"text": "", "confidence": 0}
            
            # First annotation contains all detected text
            full_text = texts[0].description
            
            # Calculate average confidence from individual words
            confidences = []
            for text in texts[1:]:  # Skip the first full text annotation
                if hasattr(text, 'confidence'):
                    confidences.append(text.confidence * 100)
            
            avg_confidence = sum(confidences) / len(confidences) if confidences else 85  # Default confidence
            
            return {
                "text": full_text,
                "confidence": avg_confidence,
                "annotations": len(texts),
                "bounding_boxes": [text.bounding_poly for text in texts[1:]]
            }
            
        except Exception as e:
            self.logger.error(f"Google Vision processing failed: {e}")
            return {"text": "", "confidence": 0, "error": str(e)}
    
    def combine_ocr_results(self, ocr_results: Dict[str, Any]) -> str:
        """
        Intelligently combine OCR results from multiple engines
        """
        texts = []
        confidences = []
        
        for engine, result in ocr_results.items():
            if engine in ['tesseract', 'aws_textract', 'google_vision'] and result and result.get('text'):
                texts.append(result['text'])
                confidences.append(result.get('confidence', 0))
        
        if not texts:
            return ""
        
        # If we have multiple results, choose the one with highest confidence
        if len(texts) > 1:
            best_idx = confidences.index(max(confidences))
            primary_text = texts[best_idx]
            
            # Enhance primary text with additional information from other OCR results
            enhanced_text = self.enhance_primary_text(primary_text, texts)
            return enhanced_text
        else:
            return texts[0]
    
    def enhance_primary_text(self, primary_text: str, all_texts: List[str]) -> str:
        """
        Enhance primary OCR result with information from other OCR engines
        """
        # For now, return the primary text
        # In future, could implement text merging algorithms
        return primary_text
    
    def determine_best_ocr_result(self, ocr_results: Dict[str, Any]) -> str:
        """Determine which OCR engine provided the best result"""
        best_engine = "tesseract"  # default
        best_confidence = 0
        
        for engine, result in ocr_results.items():
            if engine in ['tesseract', 'aws_textract', 'google_vision'] and result:
                confidence = result.get('confidence', 0)
                if confidence > best_confidence:
                    best_confidence = confidence
                    best_engine = engine
        
        return best_engine
    
    def image_to_bytes(self, image: Image.Image) -> bytes:
        """Convert PIL Image to bytes"""
        img_byte_arr = io.BytesIO()
        image.save(img_byte_arr, format='PNG')
        return img_byte_arr.getvalue()
    
    # =============== FORMAT-SPECIFIC PROCESSORS ===============
    
    def process_pdf(self, file_path: Path) -> Dict[str, Any]:
        """Extract content from PDF with multi-OCR for image-based pages"""
        try:
            doc = fitz.open(file_path)
            
            result = {
                "text_content": "",
                "pages": [],
                "images": [],
                "tables": [],
                "metadata": doc.metadata,
                "page_count": len(doc),
                "ocr_results": []
            }
            
            for page_num in range(len(doc)):
                page = doc.load_page(page_num)
                
                # Extract text
                page_text = page.get_text()
                
                # If page has little text, convert to image and OCR
                if len(page_text.strip()) < 100:
                    self.logger.info(f"Page {page_num + 1} appears to be image-based, applying OCR")
                    
                    # Convert page to image
                    pix = page.get_pixmap(matrix=fitz.Matrix(2.0, 2.0))  # 2x scale for better OCR
                    img_data = pix.tobytes("png")
                    
                    # Save temporarily and process with multi-OCR
                    temp_path = f"/tmp/temp_page_{page_num}.png"
                    with open(temp_path, "wb") as f:
                        f.write(img_data)
                    
                    ocr_result = self.process_image_with_multi_ocr(Path(temp_path))
                    page_text = ocr_result["combined_text"]
                    result["ocr_results"].append({
                        "page": page_num + 1,
                        "ocr_details": ocr_result
                    })
                    
                    # Clean up temp file
                    if os.path.exists(temp_path):
                        os.remove(temp_path)
                
                result["pages"].append({
                    "page_number": page_num + 1,
                    "text": page_text
                })
                
                result["text_content"] += page_text + "\n"
            
            doc.close()
            return result
            
        except Exception as e:
            raise Exception(f"PDF processing failed: {e}")
    
    def process_image(self, file_path: Path) -> Dict[str, Any]:
        """Process image with comprehensive multi-OCR analysis"""
        try:
            ocr_results = self.process_image_with_multi_ocr(file_path)
            
            # Additional image analysis
            image = Image.open(file_path)
            
            result = {
                "text_content": ocr_results["combined_text"],
                "image_info": {
                    "width": image.width,
                    "height": image.height,
                    "mode": image.mode,
                    "format": image.format
                },
                "ocr_results": ocr_results,
                "best_ocr_engine": ocr_results["best_result"]
            }
            
            return result
            
        except Exception as e:
            raise Exception(f"Image processing failed: {e}")
    
    # =============== STRUCTURAL ANALYSIS ===============
    
    def analyze_structure(self, extraction_result: Dict[str, Any]) -> Dict[str, Any]:
        """Analyze document structure specifically for medical rubrics"""
        text_content = extraction_result.get("text_content", "")
        
        structure_analysis = {
            "document_type": self.detect_medical_document_type(text_content),
            "sections": self.identify_medical_sections(text_content),
            "assessment_domains": self.detect_assessment_domains(text_content),
            "scoring_system": self.detect_scoring_system(text_content),
            "text_statistics": self.get_text_statistics(text_content)
        }
        
        return structure_analysis
    
    def detect_medical_document_type(self, text: str) -> str:
        """Detect medical document type with improved accuracy"""
        text_lower = text.lower()
        
        # Medical assessment patterns
        patterns = {
            "osce_rubric": [
                r"osce", r"objective.*structured.*clinical.*examination",
                r"station.*\d+", r"assessment.*criteria", r"rubric"
            ],
            "clinical_assessment": [
                r"clinical.*assessment", r"patient.*encounter", 
                r"history.*taking", r"physical.*exam"
            ],
            "medical_rubric": [
                r"rubric", r"scoring.*guide", r"assessment.*criteria",
                r"evaluation.*form", r"grading.*criteria"
            ],
            "competency_assessment": [
                r"competency", r"milestone", r"epa", r"entrustable.*professional.*activity"
            ]
        }
        
        scores = {}
        for doc_type, pattern_list in patterns.items():
            score = 0
            for pattern in pattern_list:
                matches = len(re.findall(pattern, text_lower))
                score += matches
            scores[doc_type] = score
        
        if scores and max(scores.values()) > 0:
            return max(scores, key=scores.get)
        
        return "medical_document"
    
    def identify_medical_sections(self, text: str) -> List[Dict[str, Any]]:
        """Identify medical assessment sections"""
        sections = []
        
        # Common medical assessment section patterns
        section_patterns = [
            (r"history.*taking", "History Taking"),
            (r"physical.*exam", "Physical Examination"),
            (r"diagnostic.*accuracy", "Diagnostic Accuracy"),
            (r"diagnostic.*reasoning", "Diagnostic Reasoning"),
            (r"management", "Management"),
            (r"communication", "Communication"),
            (r"professionalism", "Professionalism"),
            (r"clinical.*reasoning", "Clinical Reasoning"),
            (r"patient.*care", "Patient Care"),
            (r"interpersonal.*skills", "Interpersonal Skills")
        ]
        
        for pattern, section_name in section_patterns:
            if re.search(pattern, text, re.IGNORECASE):
                sections.append({
                    "name": section_name,
                    "detected": True,
                    "pattern_used": pattern
                })
        
        return sections
    
    def detect_assessment_domains(self, text: str) -> List[str]:
        """Detect ACGME/medical assessment domains"""
        domains = []
        domain_patterns = {
            "Patient Care": [r"patient.*care", r"clinical.*care"],
            "Medical Knowledge": [r"medical.*knowledge", r"knowledge.*application"],
            "Practice-Based Learning": [r"practice.*based.*learning", r"improvement"],
            "Interpersonal and Communication Skills": [r"communication", r"interpersonal"],
            "Professionalism": [r"professionalism", r"professional.*behavior"],
            "Systems-Based Practice": [r"systems.*based", r"healthcare.*systems"]
        }
        
        text_lower = text.lower()
        for domain, patterns in domain_patterns.items():
            if any(re.search(pattern, text_lower) for pattern in patterns):
                domains.append(domain)
        
        return domains
    
    def detect_scoring_system(self, text: str) -> Dict[str, Any]:
        """Detect the scoring system used in the rubric"""
        # Look for point values and scoring patterns
        point_patterns = [
            r"(\d+)\s*points?",
            r"(\d+)\s*pts?",
            r"(\d+)/(\d+)",
            r"score.*(\d+)",
            r"(\d+)\s*-\s*(\d+)"
        ]
        
        found_scores = []
        for pattern in point_patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            found_scores.extend(matches)
        
        # Determine scoring system type
        if any("point" in text.lower() or "pts" in text.lower() for _ in [1]):
            system_type = "points"
        elif any(str(i) for i in range(1, 6)) and any(str(i) for i in range(1, 6)):
            system_type = "scale_1_5"
        else:
            system_type = "unknown"
        
        return {
            "type": system_type,
            "detected_scores": list(set(found_scores)),
            "max_possible_score": self.extract_max_score(found_scores)
        }
    
    def extract_max_score(self, scores: List) -> Optional[int]:
        """Extract maximum possible score from detected scores"""
        try:
            numeric_scores = []
            for score in scores:
                if isinstance(score, tuple):
                    numeric_scores.extend([int(s) for s in score if s.isdigit()])
                elif isinstance(score, str) and score.isdigit():
                    numeric_scores.append(int(score))
            
            return max(numeric_scores) if numeric_scores else None
        except:
            return None
    
    # =============== MEDICAL RUBRIC ANALYSIS ===============
    
    def analyze_medical_rubric(self, extraction_result: Dict[str, Any], 
                              structure_analysis: Dict[str, Any]) -> Dict[str, Any]:
        """Comprehensive medical rubric analysis"""
        text_content = extraction_result.get("text_content", "")
        
        rubric_analysis = {
            "criteria": self.extract_assessment_criteria(text_content),
            "domains": self.map_to_medical_domains(text_content),
            "scoring_structure": self.analyze_scoring_structure(text_content),
            "competency_mapping": self.map_competencies(text_content),
            "rubric_quality": self.assess_rubric_quality(text_content)
        }
        
        return rubric_analysis
    
    def extract_assessment_criteria(self, text: str) -> List[Dict[str, Any]]:
        """Extract individual assessment criteria from rubric text"""
        criteria = []
        
        # Split text into potential criteria sections
        lines = text.split('\n')
        current_criterion = None
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
            
            # Check if this line looks like a criterion header
            if self.is_criterion_header(line):
                if current_criterion:
                    criteria.append(current_criterion)
                
                current_criterion = {
                    "name": line,
                    "description": "",
                    "points": self.extract_points_from_line(line),
                    "category": self.categorize_criterion(line),
                    "examples": [],
                    "subcriteria": []
                }
            elif current_criterion:
                # Add to description or examples
                if self.looks_like_example(line):
                    current_criterion["examples"].append(line)
                else:
                    current_criterion["description"] += " " + line
        
        if current_criterion:
            criteria.append(current_criterion)
        
        return criteria
    
    def is_criterion_header(self, line: str) -> bool:
        """Determine if a line is likely a criterion header"""
        # Look for common criterion header patterns
        patterns = [
            r"^\d+\.\s+",  # Numbered items
            r"^[A-Z][a-z\s]+:$",  # Title case with colon
            r"^[A-Z\s]{3,}$",  # All caps
            r"^\s*[-â€¢]\s+",  # Bullet points
        ]
        
        return any(re.match(pattern, line) for pattern in patterns)
    
    def extract_points_from_line(self, line: str) -> Optional[int]:
        """Extract point values from criterion lines"""
        point_patterns = [
            r"(\d+)\s*points?",
            r"(\d+)\s*pts?",
            r"\((\d+)\)",
            r"(\d+)$"
        ]
        
        for pattern in point_patterns:
            match = re.search(pattern, line, re.IGNORECASE)
            if match:
                return int(match.group(1))
        
        return None
    
    def categorize_criterion(self, criterion_text: str) -> str:
        """Categorize criterion into medical domains"""
        text_lower = criterion_text.lower()
        
        categories = {
            "History Taking": ["history", "symptoms", "complaint", "hpi", "pmh"],
            "Physical Examination": ["physical", "exam", "palpat", "auscult", "inspect"],
            "Diagnostic Reasoning": ["diagnosis", "diagnostic", "reasoning", "differential"],
            "Management": ["management", "treatment", "plan", "therapy"],
            "Communication": ["communication", "explain", "patient", "rapport"],
            "Professionalism": ["professional", "ethics", "respect", "confidentiality"]
        }
        
        for category, keywords in categories.items():
            if any(keyword in text_lower for keyword in keywords):
                return category
        
        return "General"
    
    def looks_like_example(self, line: str) -> bool:
        """Determine if a line looks like an example"""
        example_indicators = [
            r"^e\.g\.",
            r"^for example",
            r"^such as",
            r"^\s*-\s+[a-z]",  # Lowercase bullet point
            r"\".*\""  # Quoted text
        ]
        
        return any(re.search(pattern, line, re.IGNORECASE) for pattern in example_indicators)
    
    def map_to_medical_domains(self, text: str) -> Dict[str, List[str]]:
        """Map content to ACGME core competencies"""
        competency_mapping = {
            "Patient Care": [],
            "Medical Knowledge": [],
            "Practice-Based Learning and Improvement": [],
            "Interpersonal and Communication Skills": [],
            "Professionalism": [],
            "Systems-Based Practice": []
        }
        
        # Define keyword mappings for each competency
        keyword_maps = {
            "Patient Care": ["patient care", "clinical care", "treatment", "management"],
            "Medical Knowledge": ["knowledge", "diagnosis", "pathophysiology", "evidence"],
            "Practice-Based Learning and Improvement": ["improvement", "learning", "feedback", "quality"],
            "Interpersonal and Communication Skills": ["communication", "interpersonal", "team", "patient interaction"],
            "Professionalism": ["professionalism", "ethics", "integrity", "respect"],
            "Systems-Based Practice": ["systems", "healthcare delivery", "cost-effectiveness", "safety"]
        }
        
        for competency, keywords in keyword_maps.items():
            for keyword in keywords:
                if keyword.lower() in text.lower():
                    competency_mapping[competency].append(keyword)
        
        return competency_mapping
    
    def analyze_scoring_structure(self, text: str) -> Dict[str, Any]:
        """Analyze the scoring structure of the rubric"""
        return {
            "total_possible_points": self.calculate_total_points(text),
            "point_distribution": self.analyze_point_distribution(text),
            "scoring_levels": self.detect_scoring_levels(text)
        }
    
    def calculate_total_points(self, text: str) -> int:
        """Calculate total possible points from rubric"""
        point_matches = re.findall(r"(\d+)\s*(?:points?|pts?)", text, re.IGNORECASE)
        return sum(int(match) for match in point_matches)
    
    def analyze_point_distribution(self, text: str) -> Dict[str, int]:
        """Analyze how points are distributed across categories"""
        # This is a simplified implementation
        # In production, would be more sophisticated
        return {"History Taking": 25, "Physical Exam": 15, "Diagnosis": 10, "Management": 5}
    
    def detect_scoring_levels(self, text: str) -> List[str]:
        """Detect different scoring levels (e.g., Excellent, Good, Needs Improvement)"""
        level_patterns = [
            r"excellent", r"outstanding", r"exemplary",
            r"good", r"satisfactory", r"adequate",
            r"needs improvement", r"unsatisfactory", r"poor"
        ]
        
        found_levels = []
        for pattern in level_patterns:
            if re.search(pattern, text, re.IGNORECASE):
                found_levels.append(pattern.title())
        
        return found_levels
    
    def map_competencies(self, text: str) -> Dict[str, Any]:
        """Map to specific medical competencies and milestones"""
        # This would map to specific competency frameworks
        return {
            "acgme_competencies": self.map_to_medical_domains(text),
            "epa_mapping": self.map_to_epas(text),
            "milestone_level": self.estimate_milestone_level(text)
        }
    
    def map_to_epas(self, text: str) -> List[str]:
        """Map to Entrustable Professional Activities"""
        # Simplified EPA mapping
        return ["EPA 1: Gather a history and perform a physical examination"]
    
    def estimate_milestone_level(self, text: str) -> str:
        """Estimate the milestone level of the assessment"""
        # Simplified milestone estimation
        return "Novice to Advanced Beginner"
    
    def assess_rubric_quality(self, text: str) -> Dict[str, Any]:
        """Assess the quality and completeness of the rubric"""
        return {
            "completeness_score": self.calculate_completeness_score(text),
            "clarity_indicators": self.assess_clarity(text),
            "bias_check": self.check_for_bias(text),
            "recommendations": self.generate_improvement_recommendations(text)
        }
    
    def calculate_completeness_score(self, text: str) -> float:
        """Calculate a completeness score for the rubric"""
        required_elements = [
            "clear criteria", "point values", "examples", 
            "different performance levels", "objective language"
        ]
        # Simplified scoring
        return 0.85  # 85% complete
    
    def assess_clarity(self, text: str) -> List[str]:
        """Assess clarity indicators in the rubric"""
        return ["Uses specific, measurable language", "Provides clear examples"]
    
    def check_for_bias(self, text: str) -> List[str]:
        """Check for potential bias in the rubric"""
        return []  # No bias detected
    
    def generate_improvement_recommendations(self, text: str) -> List[str]:
        """Generate recommendations for improving the rubric"""
        return [
            "Consider adding more specific behavioral indicators",
            "Include examples of different performance levels"
        ]
    
    # =============== UTILITY METHODS ===============
    
    def merge_criteria(self, criteria_lists: List[List[Dict]]) -> List[Dict]:
        """Merge criteria from multiple documents"""
        merged = []
        seen_names = set()
        
        for criteria_list in criteria_lists:
            for criterion in criteria_list:
                if criterion.get("name") not in seen_names:
                    merged.append(criterion)
                    seen_names.add(criterion.get("name"))
        
        return merged
    
    def determine_dominant_structure(self, structure_types: List[str]) -> str:
        """Determine the dominant structure type from multiple documents"""
        if not structure_types:
            return "unknown"
        
        # Return the most common structure type
        from collections import Counter
        return Counter(structure_types).most_common(1)[0][0]
    
    def analyze_cross_document_patterns(self, texts: List[str]) -> Dict[str, Any]:
        """Analyze patterns across multiple documents"""
        return {
            "common_themes": self.find_common_themes(texts),
            "terminology_consistency": self.check_terminology_consistency(texts),
            "structure_similarity": self.assess_structure_similarity(texts)
        }
    
    def find_common_themes(self, texts: List[str]) -> List[str]:
        """Find common themes across documents"""
        # Simplified implementation
        return ["History Taking", "Physical Examination", "Clinical Reasoning"]
    
    def check_terminology_consistency(self, texts: List[str]) -> float:
        """Check consistency of terminology across documents"""
        # Simplified implementation
        return 0.8  # 80% consistent
    
    def assess_structure_similarity(self, texts: List[str]) -> float:
        """Assess structural similarity across documents"""
        # Simplified implementation
        return 0.75  # 75% similar structure
    
    def get_text_statistics(self, text: str) -> Dict[str, Any]:
        """Get comprehensive text statistics"""
        if not text.strip():
            return {"word_count": 0, "character_count": 0}
        
        return {
            "character_count": len(text),
            "word_count": len(text.split()),
            "line_count": len(text.splitlines()),
            "paragraph_count": len([p for p in text.split('\n\n') if p.strip()]),
            "average_words_per_line": len(text.split()) / max(1, len(text.splitlines()))
        }
    
    # =============== ADDITIONAL FORMAT PROCESSORS ===============
    
    def process_docx(self, file_path: Path) -> Dict[str, Any]:
        """Process DOCX files"""
        try:
            doc = Document(file_path)
            text_content = ""
            
            for para in doc.paragraphs:
                text_content += para.text + "\n"
            
            return {"text_content": text_content}
        except Exception as e:
            raise Exception(f"DOCX processing failed: {e}")
    
    def process_xlsx(self, file_path: Path) -> Dict[str, Any]:
        """Process Excel files"""
        try:
            df = pd.read_excel(file_path)
            text_content = df.to_string()
            
            return {"text_content": text_content}
        except Exception as e:
            raise Exception(f"Excel processing failed: {e}")
    
    def process_csv(self, file_path: Path) -> Dict[str, Any]:
        """Process CSV files"""
        try:
            df = pd.read_csv(file_path)
            text_content = df.to_string()
            
            return {"text_content": text_content}
        except Exception as e:
            raise Exception(f"CSV processing failed: {e}")
    
    def process_txt(self, file_path: Path) -> Dict[str, Any]:
        """Process text files"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                text_content = f.read()
            
            return {"text_content": text_content}
        except Exception as e:
            raise Exception(f"Text processing failed: {e}")
    
    def process_doc(self, file_path: Path) -> Dict[str, Any]:
        """Process DOC files (legacy format)"""
        return {
            "text_content": "",
            "error": "Legacy .doc format not supported. Please convert to .docx"
        }
    
    def process_xls(self, file_path: Path) -> Dict[str, Any]:
        """Process XLS files (legacy format)"""
        try:
            df = pd.read_excel(file_path, engine='xlrd')
            text_content = df.to_string()
            
            return {"text_content": text_content}
        except Exception as e:
            raise Exception(f"XLS processing failed: {e}")


def main():
    """Example usage"""
    analyzer = ComprehensiveDocumentAnalyzer(
        aws_access_key=os.getenv('AWS_ACCESS_KEY_ID'),
        aws_secret_key=os.getenv('AWS_SECRET_ACCESS_KEY'),
        google_credentials_path=os.getenv('GOOGLE_APPLICATION_CREDENTIALS')
    )
    
    # Example: Analyze multiple files
    files = [
        "/path/to/rubric1.pdf",
        "/path/to/rubric2.png",
        "/path/to/rubric3.xlsx"
    ]
    
    result = analyzer.analyze_multiple_documents(files)
    print(json.dumps(result, indent=2))


if __name__ == "__main__":
    main() 