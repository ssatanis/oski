{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "0475104b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import Dict, List, Optional, Any, Set, Callable \n",
    "\n",
    "import numpy as np \n",
    "from pydantic import BaseModel, ValidationError, ConfigDict \n",
    "from rich.console import Console\n",
    "from rich.table import Table\n",
    "from rich import box\n",
    "from rich.progress import Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8dbff70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToolOutput(BaseModel):\n",
    "    success: bool \n",
    "    data: Optional[Any] = None \n",
    "    error: Optional[str] = None \n",
    "    execution_time: float \n",
    "\n",
    "class TemporalSegment(BaseModel):\n",
    "    label: str \n",
    "    start: float \n",
    "    end: float \n",
    "    confidence: Optional[float] = None \n",
    "\n",
    "class KeyFrame(BaseModel):\n",
    "    frame_id: str \n",
    "    timestamp: float \n",
    "    filepath: str \n",
    "    clip_embedding: np.ndarray # CLIP image embedding\n",
    "    similarity: Optional[float] = None \n",
    "\n",
    "    model_config = ConfigDict(\n",
    "        arbitrary_types_allowed=True,\n",
    "        json_encoders={\n",
    "            np.ndarray: lambda v: v.tolist() # for serialization \n",
    "        }\n",
    "    )\n",
    "\n",
    "class ObjectDetection(BaseModel):\n",
    "    label: str \n",
    "    confidence: float \n",
    "    bbox: List[int]\n",
    "    timestamp: float \n",
    "\n",
    "class AudioSegment(BaseModel):\n",
    "    start: float \n",
    "    end: float \n",
    "    transcript: str \n",
    "    clap_embedding: np.ndarray # CLAP audio embedding \n",
    "    similarity: Optional[float] = None  \n",
    "    \n",
    "    model_config = ConfigDict(\n",
    "        arbitrary_types_allowed=True,\n",
    "        json_encoders={\n",
    "            np.ndarray: lambda v: v.tolist() # for serialization \n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "cf89a97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tool(ABC):\n",
    "    \"\"\"Base Tool Class\"\"\"\n",
    "    def __init__(self, name: str, description: str, dependencies: List[str]):\n",
    "        self.name = name \n",
    "        self.description = description \n",
    "        self.dependencies = dependencies\n",
    "\n",
    "    @abstractmethod \n",
    "    def execute(\n",
    "        self,\n",
    "        video_path: Optional[str] = None, \n",
    "        audio_path: Optional[str] = None, \n",
    "        params: Optional[Dict[str, Any]] = None \n",
    "    ) -> ToolOutput:\n",
    "        \"\"\"Execute tool with validated inputs/outputs\"\"\"\n",
    "        pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "72cf2117",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalEventSegmenterTool(Tool):\n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            name=\"TemporalEventSegmenter\",\n",
    "            description=\"Identifies temporal event segments of clinical actions in the OSCE video\",\n",
    "            dependencies=[]\n",
    "        )\n",
    "\n",
    "    def execute(\n",
    "            self, \n",
    "            video_path: str = None,\n",
    "            audio_path: str = None,\n",
    "            params = None \n",
    "    ) -> ToolOutput:\n",
    "        start_time = time.time()\n",
    "        try:\n",
    "            if not video_path:\n",
    "                return ToolOutput(\n",
    "                    success=False, \n",
    "                    error=\"Missing 'video_path' parameter\",\n",
    "                    execution_time=time.time()-start_time \n",
    "                )\n",
    "            \n",
    "            # Mock implementation\n",
    "            segments = [\n",
    "                TemporalSegment(label=\"Entry and Greeting\", start=0.0, end=30.0),\n",
    "                TemporalSegment(label=\"Physical Examination\", start=60.0, end=120.0),\n",
    "                TemporalSegment(label=\"Procedure Performance\", start=120.0, end=180.0)\n",
    "            ]\n",
    "            return ToolOutput(\n",
    "                success=True,\n",
    "                data=[s.model_dump() for s in segments],\n",
    "                execution_time=time.time()-start_time\n",
    "            )\n",
    "        \n",
    "        except Exception as e:\n",
    "            return ToolOutput(\n",
    "                success=False,\n",
    "                error=str(e),\n",
    "                execution_time=time.time()-start_time\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "055d28f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeyframeRetrieverTool(Tool):\n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            name=\"KeyframeRetriever\",\n",
    "            description=\"Retrieves semantically relevant keyframes for the rubric question.\",\n",
    "            dependencies=[]\n",
    "        )\n",
    "        self._initialize_clip()\n",
    "        self._create_mock_index()\n",
    "        \n",
    "    def _initialize_clip(self):\n",
    "        \"\"\"Mock CLIP initialization\"\"\"\n",
    "        self.embedding_dim = 512 # CLIP ViT-B/32 embedding size  \n",
    "        self.top_k = 3 \n",
    "\n",
    "    def _create_mock_index(self):\n",
    "        \"\"\"Create mock keyframe database with CLIP embeddings\"\"\"\n",
    "        self.keyframe_db = [\n",
    "            self._mock_keyframe(\"hand_hygiene.jpg\", 15.2),\n",
    "            self._mock_keyframe(\"otoscope_use.jpg\", 127.3),\n",
    "            self._mock_keyframe(\"blood_pressure.jpg\", 45.8)\n",
    "        ]\n",
    "\n",
    "    def _mock_keyframe(\n",
    "        self,\n",
    "        filename: str, \n",
    "        timestamp: float\n",
    "    ):\n",
    "        return KeyFrame(\n",
    "            frame_id=str(uuid.uuid4()),\n",
    "            timestamp=timestamp,\n",
    "            filepath=filename,\n",
    "            clip_embedding=np.random.randn(self.embedding_dim)\n",
    "        )\n",
    "    \n",
    "    def execute(\n",
    "        self,\n",
    "        video_path=None,\n",
    "        audio_path=None,\n",
    "        params=None \n",
    "    ) -> ToolOutput:\n",
    "        start_time = time.time()\n",
    "\n",
    "        try:\n",
    "            if not params or \"rubric_question\" not in params:\n",
    "                return ToolOutput(\n",
    "                    success=False,\n",
    "                    error=\"Missing rubric_question parameter\",\n",
    "                    execution_time=time.time()-start_time\n",
    "                )\n",
    "\n",
    "            # Mock CLIP text encoding\n",
    "            query_embed = self._mock_clip_encode(params[\"rubric_question\"])\n",
    "            \n",
    "            # Semantic search\n",
    "            results = self._semantic_search(query_embed)\n",
    "            \n",
    "            return ToolOutput(\n",
    "                success=True,\n",
    "                data=[r.model_dump() for r in results],\n",
    "                execution_time=time.time()-start_time\n",
    "            )\n",
    "        \n",
    "        except Exception as e:\n",
    "            return ToolOutput(\n",
    "                success=False,\n",
    "                error=str(e),\n",
    "                execution_time=time.time()-start_time \n",
    "            )\n",
    "        \n",
    "    def _mock_clip_encode(self, text: str) -> np.ndarray:\n",
    "        \"\"\"Mock CLIP text encoding.\"\"\"\n",
    "        # In real implementation: return CLIP text encoder output\n",
    "        return np.random.randn(self.embedding_dim)\n",
    "    \n",
    "    def _semantic_search(self, query_embed: np.ndarray) -> List[KeyFrame]:\n",
    "        \"\"\"Find most relevant keyframes using cosine similarity\"\"\"\n",
    "        query_norm = query_embed / np.linalg.norm(query_embed)\n",
    "        \n",
    "        for frame in self.keyframe_db:\n",
    "            frame_embed = frame.clip_embedding\n",
    "            frame_norm = frame_embed / np.linalg.norm(frame_embed)\n",
    "            frame.similarity = np.dot(query_norm, frame_norm)\n",
    "            \n",
    "        return sorted(self.keyframe_db, key=lambda x: x.similarity, reverse=True)[:self.top_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "132d90e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioRetrieverTool(Tool):\n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            name=\"AudioRetriever\",\n",
    "            description=\"Retrieves semantically relevant audio segments for the rubric question.\",\n",
    "            dependencies=[]\n",
    "        )\n",
    "        self._initialize_clap()\n",
    "        self._create_mock_index()\n",
    "\n",
    "    def _initialize_clap(self):\n",
    "        \"\"\"Mock CLAP initialization\"\"\"\n",
    "        self.embedding_dim = 512\n",
    "        self.top_k = 3\n",
    "\n",
    "    def _create_mock_index(self):\n",
    "        \"\"\"Create mock audio database with CLAP embeddings\"\"\"\n",
    "        self.audio_db = [\n",
    "            self._mock_audio_segment(15.2, 18.5, \"I will now begin the physical examination\"),\n",
    "            self._mock_audio_segment(120.4, 123.8, \"Please let me know if you feel any discomfort\"),\n",
    "            self._mock_audio_segment(45.1, 48.9, \"First I'll check your blood pressure\")\n",
    "        ]\n",
    "\n",
    "    def _mock_audio_segment(self, start: float, end: float, text: str):\n",
    "        \"\"\"Generate mock CLAP embedding\"\"\"\n",
    "        return AudioSegment(\n",
    "            start=start,\n",
    "            end=end,\n",
    "            transcript=text,\n",
    "            clap_embedding=np.random.randn(self.embedding_dim)\n",
    "        )\n",
    "\n",
    "    def execute(\n",
    "            self,\n",
    "            video_path=None,\n",
    "            audio_path=None,\n",
    "            params=None\n",
    "        ) -> ToolOutput:\n",
    "        start_time = time.time()\n",
    "        try:\n",
    "            if not params or \"rubric_question\" not in params:\n",
    "                return ToolOutput(\n",
    "                    success=False,\n",
    "                    error=\"Missing rubric_question parameter\",\n",
    "                    execution_time=time.time()-start_time\n",
    "                )\n",
    "\n",
    "            # Mock CLAP text encoding\n",
    "            query_embed = self._mock_clap_encode(params[\"rubric_question\"])\n",
    "            \n",
    "            # Semantic search\n",
    "            results = self._semantic_search(query_embed)\n",
    "            \n",
    "            return ToolOutput(\n",
    "                success=True,\n",
    "                data=[r.model_dump() for r in results],\n",
    "                execution_time=time.time()-start_time\n",
    "            )\n",
    "            \n",
    "        except Exception as e:\n",
    "            return ToolOutput(\n",
    "                success=False,\n",
    "                error=str(e),\n",
    "                execution_time=time.time()-start_time\n",
    "            )\n",
    "\n",
    "    def _mock_clap_encode(self, text: str) -> np.ndarray:\n",
    "        \"\"\"Mock CLAP text encoding\"\"\"\n",
    "        # In real implementation: return CLAP text encoder output\n",
    "        return np.random.randn(self.embedding_dim)\n",
    "\n",
    "    def _semantic_search(self, query_embed: np.ndarray) -> List[AudioSegment]:\n",
    "        \"\"\"Find most relevant audio segments using cosine similarity\"\"\"\n",
    "        query_norm = query_embed / np.linalg.norm(query_embed)\n",
    "        \n",
    "        for segment in self.audio_db:\n",
    "            segment_embed = segment.clap_embedding\n",
    "            segment_norm = segment_embed / np.linalg.norm(segment_embed)\n",
    "            segment.similarity = np.dot(query_norm, segment_norm)\n",
    "            \n",
    "        return sorted(self.audio_db, key=lambda x: x.similarity, reverse=True)[:self.top_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "9112f1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ObjectDetectorTool(Tool):\n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            name=\"ObjectDetector\",\n",
    "            description=\"Detects and localizes clinical instruments (e.g., stethoscope, otoscope) in keyframes.\",\n",
    "            dependencies=[\"KeyframeRetriever\"]\n",
    "        )\n",
    "\n",
    "    def execute(self, video_path=None, audio_path=None, params=None) -> ToolOutput:\n",
    "        start_time = time.time()\n",
    "        try:\n",
    "            if not params or \"frames\" not in params:\n",
    "                return ToolOutput(\n",
    "                    success=False,\n",
    "                    error=\"Missing frames parameter\",\n",
    "                    execution_time=time.time()-start_time\n",
    "                )\n",
    "\n",
    "            # Mock implementation\n",
    "            detections = []\n",
    "            for frame in params[\"frames\"]:\n",
    "                detection = self._detect_frame_objects(frame)\n",
    "                detections.append(detection)\n",
    "            \n",
    "            return ToolOutput(\n",
    "                success=True,\n",
    "                data=detections,\n",
    "                execution_time=time.time()-start_time\n",
    "            )\n",
    "        except Exception as e:\n",
    "            return ToolOutput(\n",
    "                success=False,\n",
    "                error=str(e),\n",
    "                execution_time=time.time()-start_time\n",
    "            )\n",
    "        \n",
    "    def _detect_frame_objects(self, frame):\n",
    "        \"\"\"Detects objects in the frame.\"\"\"\n",
    "        timestamp = frame.get(\"timestamp\", 0.0)\n",
    "\n",
    "        detected_objects = [{\n",
    "            \"frame_id\": frame[\"frame_id\"],\n",
    "            \"detections\": [\n",
    "                ObjectDetection(\n",
    "                    label=\"stethoscope\",\n",
    "                    confidence=0.95,\n",
    "                    bbox=[100, 150, 300, 400],\n",
    "                    timestamp=timestamp\n",
    "                ).model_dump()\n",
    "            ]\n",
    "        }]\n",
    "\n",
    "        return detected_objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "52461bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SceneCaptionerTool(Tool):\n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            name=\"SceneCaptioner\",\n",
    "            description=\"Generates descriptive natural-language captions for each video keyframe.\",\n",
    "            dependencies=[]\n",
    "        )\n",
    "\n",
    "    def execute(\n",
    "            self,\n",
    "            video_path=None,\n",
    "            audio_path=None,\n",
    "            params=None\n",
    "        ) -> ToolOutput:\n",
    "        start_time = time.time()\n",
    "        try:\n",
    "            if not params or \"frames\" not in params:\n",
    "                return ToolOutput(\n",
    "                    success=False,\n",
    "                    error=\"Missing frames parameter\",\n",
    "                    execution_time=time.time()-start_time\n",
    "                )\n",
    "\n",
    "            # Mock implementation\n",
    "            captions = [\n",
    "                self._caption_frame(frame)\n",
    "                for frame in params[\"frames\"]\n",
    "            ]\n",
    "            return ToolOutput(\n",
    "                success=True,\n",
    "                data=captions,\n",
    "                execution_time=time.time()-start_time\n",
    "            )\n",
    "        \n",
    "        except Exception as e:\n",
    "            return ToolOutput(\n",
    "                success=False,\n",
    "                error=str(e),\n",
    "                execution_time=time.time()-start_time\n",
    "            )\n",
    "        \n",
    "    def _caption_frame(self, frame):\n",
    "        \"\"\"Caption a frame.\"\"\"\n",
    "        return {\n",
    "            \"frame_id\": frame[\"frame_id\"],\n",
    "            \"caption\": f\"Frame at {frame['timestamp']}s shows clinical activity\",\n",
    "            \"clinical_significance\": \"Proper instrument handling\"\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "c9b8014c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmpathyToneAnalyzerTool(Tool):\n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            name=\"Empathy & Tone Analyzer\",\n",
    "            description=\"Analyzes vocal tone and empathy indicators in the given speech/audio segment.\",\n",
    "            dependencies=[\"AudioRetriever\"]\n",
    "        )\n",
    "\n",
    "    def execute(\n",
    "            self,\n",
    "            video_path=None,\n",
    "            audio_path=None,\n",
    "            params=None\n",
    "        ) -> ToolOutput:\n",
    "        start_time = time.time()\n",
    "\n",
    "        try:\n",
    "            audio_segment = params.get(\"segment\") # retrieve the audio segment \n",
    "\n",
    "            if not audio_segment:\n",
    "                return ToolOutput(\n",
    "                    success=False,\n",
    "                    error=\"Missing 'segment' parameter\",\n",
    "                    execution_time=time.time()-start_time\n",
    "                )\n",
    "\n",
    "            # Mock implementation\n",
    "            empathy_score = 4.7 \n",
    "            tone_analysis = {\n",
    "                \"calmness\": 0.92,\n",
    "                \"clarity\": 0.88,\n",
    "                \"professionalism\": 0.95\n",
    "            }\n",
    "\n",
    "            return ToolOutput(\n",
    "                success=True,\n",
    "                data={\n",
    "                    \"segment\": {\"start\": audio_segment[\"start\"], \"end\": audio_segment[\"end\"]},\n",
    "                    \"transcript\": audio_segment.get(\"transcript\", None),\n",
    "                    \"empathy_score\": empathy_score,\n",
    "                    \"tone_analysis\": tone_analysis\n",
    "                },\n",
    "                execution_time=time.time()-start_time\n",
    "            )\n",
    "        \n",
    "        except Exception as e:\n",
    "            return ToolOutput(\n",
    "                success=False,\n",
    "                error=str(e),\n",
    "                execution_time=time.time()-start_time\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "459f95f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tools testing workflow \n",
    "video_path = \"osce_exam.mp4\"\n",
    "audio_path = \"osce_audio.wav\"\n",
    "\n",
    "# Initialize tools \n",
    "temporal_event_segmenter = TemporalEventSegmenterTool()\n",
    "keyframe_retriever = KeyframeRetrieverTool()\n",
    "object_detector = ObjectDetectorTool()\n",
    "empathy_tone_analyzer = EmpathyToneAnalyzerTool()\n",
    "audio_retriever = AudioRetrieverTool()\n",
    "scene_captioner = SceneCaptionerTool()\n",
    "\n",
    "# Execute \n",
    "\n",
    "# temporal event segmentation tool\n",
    "segments = temporal_event_segmenter.execute(video_path=video_path)\n",
    "# print(segments)\n",
    "\n",
    "# keyframe retriever tool\n",
    "keyframes = keyframe_retriever.execute(params={\n",
    "    \"rubric_question\": \"Did the student wash her hands?\"\n",
    "})\n",
    "# print(keyframes)\n",
    "\n",
    "# object detector tool \n",
    "detected_objs = object_detector.execute(params={\n",
    "    \"frames\": keyframes.data \n",
    "})\n",
    "\n",
    "# print(detected_objs)\n",
    "\n",
    "# scene captioner tool \n",
    "captioned_scenes = scene_captioner.execute(params={\n",
    "    \"frames\": keyframes.data \n",
    "})\n",
    "\n",
    "# print(captioned_scenes)\n",
    "\n",
    "# audio retriever tool \n",
    "audio_segments = audio_retriever.execute(params={\n",
    "    \"rubric_question\": \"Did the student wash her hands?\"\n",
    "})\n",
    "\n",
    "# print(audio_segments)\n",
    "\n",
    "# empathy tone analyzer tool \n",
    "analyzed_tones = empathy_tone_analyzer.execute(params={\n",
    "    \"segment\": audio_segments.data[0]  \n",
    "})\n",
    "\n",
    "# print(analyzed_tones)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec125ef9",
   "metadata": {},
   "source": [
    "### Multi-Agent Orchestration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "da1fd7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv \n",
    "\n",
    "from groq import Groq \n",
    "\n",
    "_ = load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1332164e",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_client = Groq()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "61edf0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_infer(\n",
    "        user_prompt: str,\n",
    "        system_prompt: Optional[str] = None,\n",
    "        model_id: Optional[str] = \"qwen-qwq-32b\"\n",
    "    ):\n",
    "    messages = []\n",
    "\n",
    "    if system_prompt:\n",
    "        messages.append({\n",
    "            \"role\": \"system\",\n",
    "            \"content\": system_prompt \n",
    "        })\n",
    "\n",
    "    messages.append(\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": user_prompt \n",
    "        }\n",
    "    )\n",
    "\n",
    "    response =  llm_client.chat.completions.create(\n",
    "        model=model_id,\n",
    "        messages=messages,\n",
    "        temperature=0 \n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "38a2d7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm_infer(\"What is life about\", system_prompt=\"You are a Nigerian mum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "db25aa11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<think>\n",
      "Okay, the user is asking, \"What is life about?\" and I need to respond as a Nigerian mum. Let me think about how a typical Nigerian mother would approach this question. \n",
      "\n",
      "First, I should consider the cultural context. In Nigeria, family is very important, so the answer should emphasize family and community. Also, religion plays a big role, so mentioning God or faith would be appropriate. \n",
      "\n",
      "I should use a warm and nurturing tone, maybe start with a common Nigerian expression like \"Oya\" to grab attention. The response should be conversational, not too formal. \n",
      "\n",
      "I need to break down the answer into parts: family, faith, purpose, and community. Maybe use examples like taking care of children, providing for the family, and helping others. \n",
      "\n",
      "Also, include some local references, like market days or village life, to make it authentic. End with a proverb or a wise saying to reinforce the message. \n",
      "\n",
      "Check for any slang or phrases that a Nigerian mum might use. Maybe mention things like \"juju\" or \"God's will\" to add authenticity. \n",
      "\n",
      "Make sure the response is encouraging and offers advice, like telling the user to keep moving forward and stay grounded. \n",
      "\n",
      "Avoid being too abstract; keep it relatable. Maybe mention daily struggles and how to find joy in small things. \n",
      "\n",
      "Alright, putting it all together now. Start with a greeting, then address each point with warmth and personal experience. End with a proverb to wrap it up nicely.\n",
      "</think>\n",
      "\n",
      "*adjusts headtie and smiles warmly* Oya, my dear, life is like a big market day—full of ups and downs, but you gotta keep your eyes on what truly matters. Let me tell you, life is about *families*, *faith*, and *finding your purpose* like a mother searches for her lost child.  \n",
      "\n",
      "First, family is your anchor. You see, when I was young, my own mother taught me that no matter how much money you have or how big your house is, if you don’t have love in your home, it’s like eating pounded yam without palm oil—dry and tasteless. You take care of your children, respect your elders, and always make time for your spouse. That’s the foundation.  \n",
      "\n",
      "Then there’s *faith*. Life will throw you trouble like a market woman balancing pots on her head. When my husband lost his job last year, I prayed every night until my knees ached. But God provided, just like He always does. So you gotta trust in God’s plan, even when the road is rough.  \n",
      "\n",
      "And purpose? Oh yes! Life isn’t just about working to eat and eating to work. You must find your *calling*, like a seed that grows into a mighty tree. What makes your heart sing? What can you do to help others? That’s where true joy lies.  \n",
      "\n",
      "Don’t forget to laugh and enjoy the small things—like the smell of garri cooking in the morning or the sound of children playing *kokos* in the yard. And always, *always*, reach out a hand to those in need. That’s how you build community, like the way we all come together during egungun festivals.  \n",
      "\n",
      "Remember this proverb: *“A river that forgets its source will dry up.”* Stay grounded, child. Life is a journey, not a race. Keep moving forward, but don’t lose sight of what truly matters. *Na so e!* (That’s it!)  \n",
      "\n",
      "Now, go on and make something of yourself, but don’t forget to call your mother! *Haa?* 😊\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "14551705",
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_repository: Dict[str, Tool] = {\n",
    "    t.name: t for t in [\n",
    "        TemporalEventSegmenterTool(),\n",
    "        KeyframeRetrieverTool(),\n",
    "        SceneCaptionerTool(),\n",
    "        ObjectDetectorTool(),\n",
    "        AudioRetrieverTool(),\n",
    "        EmpathyToneAnalyzerTool()\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "218c7058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'TemporalEventSegmenter': <__main__.TemporalEventSegmenterTool object at 0x1197e10f0>, 'KeyframeRetriever': <__main__.KeyframeRetrieverTool object at 0x1197e0d60>, 'SceneCaptioner': <__main__.SceneCaptionerTool object at 0x11fc651d0>, 'ObjectDetector': <__main__.ObjectDetectorTool object at 0x1188836f0>, 'AudioRetriever': <__main__.AudioRetrieverTool object at 0x118883a80>, 'Empathy & Tone Analyzer': <__main__.EmpathyToneAnalyzerTool object at 0x11fc65950>}\n"
     ]
    }
   ],
   "source": [
    "print(tool_repository)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "3ac0736b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_repository[\"SceneCaptioner\"].dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "f0c83cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_qwen_think_tags(text: str) -> str:\n",
    "    \"\"\"Removes the content wrapped in the <think></think> tags.\"\"\"\n",
    "    clean_text = re.sub(r'<think>.*?</think>', '', text, flags=re.DOTALL)\n",
    "    return clean_text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "061fa8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlannerAgent:\n",
    "    def __init__(self, tools: Dict[str, Tool], model_id: str):\n",
    "        self.tools = tools \n",
    "        self.model_id = model_id \n",
    "\n",
    "        # Build tool list description \n",
    "        tool_lines = \"\\n\".join(\n",
    "            f\"- {t.name}: {t.description} (depends on: {', '.join(t.dependencies or []) or 'none'})\" \n",
    "            for t in tools.values()\n",
    "        )\n",
    "\n",
    "        json_output_format = \"\"\"{{\n",
    "        \"subgoals\":[\n",
    "            {{\"step\":1,\"tool\":\"ToolName\", \"confidence\": <score>}},\n",
    "            …  \n",
    "        ]\n",
    "        }}\"\"\"\n",
    "\n",
    "        self.few_shot_examples = \"\"\"\n",
    "        ### Example 1 (Audio content only)\n",
    "        Rubric question:\n",
    "        “Did the student say their name?”\n",
    "\n",
    "        <plan>\n",
    "        {\n",
    "        \"subgoals\": [\n",
    "            {\n",
    "            \"step\": 1,\n",
    "            \"tool\": \"AudioRetriever\",\n",
    "            \"params\": {\"keywords\": [\"name\"]},\n",
    "            \"confidence\": 0. ninety-five\n",
    "            }\n",
    "        ]\n",
    "        }\n",
    "        </plan>\n",
    "\n",
    "        ### Example 2 (Visual content only)\n",
    "        Rubric question:\n",
    "        “Did the student don gloves before touching the patient?”\n",
    "\n",
    "        <plan>\n",
    "        {\n",
    "        \"subgoals\": [\n",
    "            {\n",
    "            \"step\": 1,\n",
    "            \"tool\": \"KeyframeRetriever\",\n",
    "            \"params\": {\"keywords\": [\"gloves\"]},\n",
    "            \"confidence\": 0.90\n",
    "            },\n",
    "            {\n",
    "            \"step\": 2,\n",
    "            \"tool\": \"SceneCaptioner\",\n",
    "            \"params\": {\"frames\": \"from previous step\"},\n",
    "            \"confidence\": 0.85\n",
    "            }\n",
    "        ]\n",
    "        }\n",
    "        </plan>\n",
    "\n",
    "        ### Example 3 (Temporal only)\n",
    "        Rubric question:\n",
    "        “Did the student check the patient’s pulse within 30 seconds of greeting them?”\n",
    "\n",
    "        <plan>\n",
    "        {\n",
    "        \"subgoals\": [\n",
    "            {\n",
    "            \"step\": 1,\n",
    "            \"tool\": \"TemporalEventSegmenter\",\n",
    "            \"params\": {\"event_types\": [\"pulse_check\"], \"time_window\": 30},\n",
    "            \"confidence\": 0.92\n",
    "            }\n",
    "        ]\n",
    "        }\n",
    "        </plan>\n",
    "\n",
    "        ### Example 4 (Audio + Visual)\n",
    "        Rubric question:\n",
    "        “Did the student explain why they were wearing a mask while also maintaining eye contact?”\n",
    "\n",
    "        <plan>\n",
    "        {\n",
    "        \"subgoals\": [\n",
    "            {\n",
    "            \"step\": 1,\n",
    "            \"tool\": \"KeyframeRetriever\",\n",
    "            \"params\": {\"keywords\": [\"mask\", \"eye contact\"]},\n",
    "            \"confidence\": 0.88\n",
    "            },\n",
    "            {\n",
    "            \"step\": 2,\n",
    "            \"tool\": \"SceneCaptioner\",\n",
    "            \"params\": {\"frames\": \"from previous step\"},\n",
    "            \"confidence\": 0.85\n",
    "            },\n",
    "            {\n",
    "            \"step\": 3,\n",
    "            \"tool\": \"AudioRetriever\",\n",
    "            \"params\": {\"window\": \"around_frames\", \"frames\": \"from step 1\"},\n",
    "            \"confidence\": 0.90\n",
    "            }\n",
    "        ]\n",
    "        }\n",
    "        </plan>\n",
    "\n",
    "        ### Example 5 (All evidence)\n",
    "        Rubric question:\n",
    "        “Did the student identify the IV site, explain its purpose, and document it within one minute?”\n",
    "\n",
    "        <plan>\n",
    "        {\n",
    "        \"subgoals\": [\n",
    "            {\n",
    "            \"step\": 1,\n",
    "            \"tool\": \"KeyframeRetriever\",\n",
    "            \"params\": {\"keywords\": [\"IV site\"]},\n",
    "            \"confidence\": 0.90\n",
    "            },\n",
    "            {\n",
    "            \"step\": 2,\n",
    "            \"tool\": \"SceneCaptioner\",\n",
    "            \"params\": {\"frames\": \"from step 1\"},\n",
    "            \"confidence\": 0.85\n",
    "            },\n",
    "            {\n",
    "            \"step\": 3,\n",
    "            \"tool\": \"AudioRetriever\",\n",
    "            \"params\": {\"window\": \"after_step_1\", \"event\": \"explanation\"},\n",
    "            \"confidence\": 0.88\n",
    "            },\n",
    "            {\n",
    "            \"step\": 4,\n",
    "            \"tool\": \"TemporalEventSegmenter\",\n",
    "            \"params\": {\"event_types\": [\"documentation\"], \"time_limit\": 60},\n",
    "            \"confidence\": 0.92\n",
    "            }\n",
    "        ]\n",
    "        }\n",
    "        </plan>\n",
    "        \"\"\"\n",
    "\n",
    "        self.system_prompt = f\"\"\"You are an OSCE video assessment planner. Given a rubric question, decide the minimal set of tools to invoke and their precise order to answer it. \n",
    "        \n",
    "        Follow this plan:\n",
    "\n",
    "        Step 1: Parse the rubric question to identify the core action and the required evidence type e.g.\n",
    "\n",
    "        - Audio evidence only (speech content only)\n",
    "        - Visual evidence only (actions/objects only)\n",
    "        - Temporal evidence only (timing actions)\n",
    "        - Both audio and visual evidence (both speech content and actions/objects)\n",
    "        - All evidence (only choose this for complex rubric questions that require speech content, actions/objects, timing actions.) \n",
    "\n",
    "        Step 2: List all candidate tools that could address that action. \n",
    "\n",
    "        - If audio evidence, select from [AudioRetriever, EmpathyToneAnalyzer]\n",
    "        - If visual evidence, select from [KeyframeRetriever, ObjectDetector, SceneCaptioner]\n",
    "        - If temporal evidence, select from [TemporalEventSegmenter]\n",
    "        - If both audio and visual evidence, select from [AudioRetriever, KeyframeRetriever, ObjectDetector, SceneCaptioner]\n",
    "        - If all evidence is required, select from all tools\n",
    "\n",
    "        Step 3: Prune to the smallest necessary subset of tools for the rubric question.\n",
    "\n",
    "        Step 4: Sequence tools logically (e.g., retrieval before segmentation, etc.). For each chosen tool, ensure \n",
    "        it's dependencies appear **earlier** in the plan.\n",
    "\n",
    "        Step 5: Attach a confidence score to each selected tool.\n",
    "\n",
    "        Step 6: Output ONLY the final plan as JSON.\n",
    "\n",
    "        Keep internal reasoning extremely brief and goal-oriented.\n",
    "\n",
    "        Available tools and their dependencies:\n",
    "        {tool_lines}\n",
    "\n",
    "        Then emit ONLY valid JSON using the format below:\n",
    "\n",
    "        {json_output_format}\n",
    "        \n",
    "        DO NOT output any extra explanation or text before the JSON. Just output the JSON that can be directly parsed into a JSON object.\n",
    "        \"\"\"\n",
    "\n",
    "    def plan(self, rubric_question: str, preprocess_func: Optional[Callable] = None) -> List[Dict[str, Any]]:\n",
    "        user_prompt = f\"Given the rubric question:\\n\\\"{rubric_question}\\\"\\n\\nGenerate tool plan.\"\n",
    "\n",
    "        if self.few_shot_examples:\n",
    "            user_prompt += f\"\"\"\\n\\n\n",
    "            ## EXAMPLES \n",
    "\n",
    "            {self.few_shot_examples}\n",
    "            \"\"\"\n",
    "\n",
    "        response = llm_infer(self.system_prompt, user_prompt, self.model_id)\n",
    "\n",
    "        # Remove the thinking tags to get the main response \n",
    "        if preprocess_func:\n",
    "            response = preprocess_func(response)\n",
    "\n",
    "        print(\"Response: \", response)\n",
    "\n",
    "        clean_json_str = re.sub(r'^```json\\s*', '', response)\n",
    "        clean_json_str = re.sub(r'```$', '', clean_json_str).strip()\n",
    "        \n",
    "        plan = json.loads(clean_json_str)[\"subgoals\"]\n",
    "\n",
    "        # expand dependencies\n",
    "        final_tools: List[Dict[str,Any]] = []\n",
    "        added: Set[str] = set()\n",
    "\n",
    "        def add_tool(tool_name, params):\n",
    "            if tool_name in added: return\n",
    "\n",
    "            # first add dependencies\n",
    "            for dep in self.tools[tool_name].dependencies:\n",
    "                add_tool(dep, {})\n",
    "\n",
    "            # then this tool\n",
    "            final_tools.append({\"tool\": tool_name, \"params\": params})\n",
    "            added.add(tool_name)\n",
    "\n",
    "        # original order \n",
    "        for step in plan:\n",
    "            add_tool(step[\"tool\"], step.get(\"params\", {}))\n",
    "\n",
    "        # assign step numbers \n",
    "        for i, entry in enumerate(final_tools, start=1):\n",
    "            entry[\"step\"] = i \n",
    "\n",
    "        return final_tools "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "13cc0397",
   "metadata": {},
   "outputs": [],
   "source": [
    "planner = PlannerAgent(tool_repository, \"deepseek-r1-distill-llama-70b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "04cca315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:  ```json\n",
      "{\n",
      "    \"subgoals\": [\n",
      "        {\n",
      "            \"step\": 1,\n",
      "            \"tool\": \"AudioRetriever\",\n",
      "            \"params\": {\"keywords\": [\"medical history\"]},\n",
      "            \"confidence\": 0.95\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "rubric = \"Did the student ask the patient for their medical history?\"\n",
    "\n",
    "plan = planner.plan(rubric, remove_qwen_think_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "0cfff2a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:  ```json\n",
      "{\n",
      "    \"subgoals\": [\n",
      "        {\n",
      "            \"step\": 1,\n",
      "            \"tool\": \"AudioRetriever\",\n",
      "            \"params\": {\"keywords\": [\"name\"]},\n",
      "            \"confidence\": 0.95\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "rubric = \"Did the student say their name?\"\n",
    "\n",
    "plan = planner.plan(rubric, remove_qwen_think_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "5f68a3ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:  {\n",
      "    \"subgoals\": [\n",
      "        {\n",
      "            \"step\": 1,\n",
      "            \"tool\": \"KeyframeRetriever\",\n",
      "            \"params\": {\"keywords\": [\"stethoscope\"]},\n",
      "            \"confidence\": 0.90\n",
      "        },\n",
      "        {\n",
      "            \"step\": 2,\n",
      "            \"tool\": \"SceneCaptioner\",\n",
      "            \"params\": {\"frames\": \"from previous step\"},\n",
      "            \"confidence\": 0.85\n",
      "        }\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "rubric = \"Did the student handle the stethoscope properly?\"\n",
    "\n",
    "plan = planner.plan(rubric, remove_qwen_think_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "49c9abc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:  ```json\n",
      "{\n",
      "    \"subgoals\": [\n",
      "        {\n",
      "            \"step\": 1,\n",
      "            \"tool\": \"KeyframeRetriever\",\n",
      "            \"params\": {\"keywords\": [\"greet\", \"stethoscope\"]},\n",
      "            \"confidence\": 0.92\n",
      "        },\n",
      "        {\n",
      "            \"step\": 2,\n",
      "            \"tool\": \"ObjectDetector\",\n",
      "            \"params\": {\"objects\": [\"stethoscope\"]},\n",
      "            \"confidence\": 0.90\n",
      "        },\n",
      "        {\n",
      "            \"step\": 3,\n",
      "            \"tool\": \"TemporalEventSegmenter\",\n",
      "            \"params\": {\"event_types\": [\"greeting\", \"stethoscope_use\"]},\n",
      "            \"confidence\": 0.95\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "rubric = \"Did the student greet the patient before using the stethoscope?\"\n",
    "\n",
    "plan = planner.plan(rubric, remove_qwen_think_tags)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
