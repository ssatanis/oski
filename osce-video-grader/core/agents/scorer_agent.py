import json 
import traceback 
from typing import List, Dict, Any, Annotated  

from google import genai 
from pydantic import BaseModel, Field

from core.utils.gemini_utils import generate_text_content_gemini_with_retry

class ScorerOutput(BaseModel):
    grade: Annotated[int, Field(strict=True, ge=0)]
    rationale: str = Field(..., min_length=10)

class Scorer:
    SCORING_CRITERIA = """
    Assign a score between 0 and 5 based on the following criteria:

    - 5 (Outstanding): The student performs the exam correctly and thoroughly. Clear evidence of proper technique in both visual descriptions and audio transcripts. Effective communication with the patient, including explanations of the procedure and findings. No observable errors or omissions in the process.
    - 4 (Very Good): Correct performance with minor lapses in technique or communication. Strong evidence of proper technique in either visual descriptions or audio transcripts, but not both. Communication is mostly clear but may lack some detail or empathy. Minor omissions or inefficiencies in the process.
    - 3 (Satisfactory): Adequate performance with noticeable gaps in technique or communication. Evidence of the exam being performed, but the quality of execution is inconsistent. Communication is present but lacks clarity or professionalism. Moderate errors or omissions in the process.
    - 2 (Needs Improvement): Weak or incomplete evidence of proper technique in visual descriptions or audio transcripts. Communication is unclear, unprofessional, or absent. Significant errors or omissions in the process.
    - 1 (Poor): The student makes minimal effort to perform the exam. Very weak evidence of the exam being performed, if any. Communication is severely lacking or entirely absent. Major errors or omissions in the process.
    - 0 (Not Performed): No evidence of the exam being performed. Neither visual descriptions nor audio transcripts indicate that the exam was attempted. Complete absence of communication about the exam.
    """

    def __init__(
        self, 
        gemini_client: genai.Client 
    ):
        self.gemini_client = gemini_client 

    def _format_evidence_for_prompt(self, evidence_from_executor: Dict[str, List[Dict[str, Any]]]) -> str:
        """
        Formats the executor's evidence into a readable string for the LLM, including full content.
        """
        formatted_evidence_parts = []
        if not evidence_from_executor:
            return "No evidence was generated by the tools."

        for tool_name, evidence_items in evidence_from_executor.items():
            part = f"Tool: {tool_name}\n"

            if not isinstance(evidence_items, list) or not evidence_items:
                part += "  - No specific evidence items found or tool did not produce output.\n"
            else:
                max_items_per_tool = 5  # Still limit the number of *items* shown per tool
                items_shown = 0
                for i, item in enumerate(evidence_items):
                    if items_shown >= max_items_per_tool:
                        part += f"  - ... (and {len(evidence_items) - max_items_per_tool} more items not shown due to brevity)\n"
                        break
                    
                    item_str = "  - "
                    if tool_name == "audio_transcript_extractor":
                        item_str += f"Transcript ({item.get('start_time', 'N/A')}s - {item.get('end_time', 'N/A')}s): \"{item.get('transcript', '')}\"" # Full transcript
                    elif tool_name == "keyframe_captioner":
                        item_str += f"Caption (Timestamp: {item.get('timestamp', 'N/A')}s): \"{item.get('keyframe_description', '')}\"" # Full description
                    elif tool_name == "object_detector":
                        item_str += f"Object: '{item.get('name', 'N/A')}' (Timestamp: {item.get('timestamp', 'N/A')}s, Conf: {item.get('confidence_score', 'N/A'):.2f}, Context: {item.get('context', '')})" # Full context
                    elif tool_name == "pose_analyzer":
                        item_str += f"Pose - {item.get('person_label', 'N/A')} (Timestamp: {item.get('timestamp', 'N/A')}s): '{item.get('pose', '')}', Gaze: '{item.get('gaze', '')}'" # Full pose and gaze
                    elif tool_name == "scene_interaction_analyzer":
                        item_str += f"Interaction (Timestamp: {item.get('timestamp', 'N/A')}s): Subject='{item.get('subject_label', 'N/A')}', Action='{item.get('action_predicate', 'N/A')}', Object='{item.get('object_target_label', 'N/A')}' (Detail: {item.get('target_detail', 'None')})" # Full detail
                    elif tool_name == "temporal_action_segmenter":
                        # Assuming action_label is descriptive enough and not excessively long
                        item_str += f"Action: '{item.get('action_label', 'N/A')}' ({item.get('start_time', 'N/A')}s - {item.get('end_time', 'N/A')}s)"
                    else:
                        # Generic representation for other/unknown tools - full JSON dump of the item
                        item_str += json.dumps(item, default=str, ensure_ascii=False)
                    part += item_str + "\n"
                    items_shown += 1
            formatted_evidence_parts.append(part)
        
        return "\n".join(formatted_evidence_parts)

    def _construct_prompt(
        self, 
        rubric_question: str, 
        formatted_evidence: str
    ) -> str:
        prompt = f"""
        You are an AI assistant acting as the 'Scorer' in a Planner-Executor-Scorer-Reflector multi-agent system for automated OSCE video assessment.
        Your role is to evaluate a student's performance on a specific task from an OSCE, based on evidence collected by automated tools (the 'Executor's' output).
        The 'Planner' selected the tools, the 'Executor' ran them, and now you must score the performance. Your output will be reviewed by a 'Reflector' agent.

        Your Task:
        1.  Carefully review the `Rubric Question` below. This is what the student is being assessed on.
        2.  Thoroughly analyze the `Evidence from Automated Analysis Tools`. This is the *only* information you should use for scoring.
        3.  Apply the `Scoring Criteria` to determine an appropriate grade.
        4.  Provide a concise `Rationale` for your grade, directly referencing specific pieces of evidence (including timestamps where available and relevant).

        Chain of Thought Instructions (Follow these steps to arrive at your answer):
        1.  **Understand the Rubric Question:** What specific skills or actions are being assessed?
        2.  **Scan Evidence for Relevance:** Identify which pieces of evidence (if any) directly address the rubric question. Note any tool errors or lack of evidence for key aspects.
        3.  **Evaluate Evidence Quality:** For relevant evidence, assess its strength and clarity. For example, is a transcript clear? Is an object detection confident? Is a temporal segment precise?
        4.  **Map Evidence to Scoring Criteria:** Compare the findings from the evidence against each level of the scoring criteria (0-5).
            *   Does the evidence show outstanding performance (5)? Or very good (4)?
            *   Are there gaps, inconsistencies, or errors indicated by the evidence that would lower the score?
            *   Is there a complete lack of evidence for the task (0)?
        5.  **Determine Grade:** Select the single integer grade (0-5) that best fits.
        6.  **Formulate Rationale:**
            *   Start by stating the overall assessment in relation to the rubric question.
            *   Justify the grade by citing specific positive and negative points from the evidence. Focus on *what* the evidence shows and *when* it occurred.
            *   **Examples of citing evidence naturally:**
                *   **Audio Transcript:** "The student explained the procedure clearly, as heard in the audio transcript from 10.5s to 12.0s where they stated, 'I will now check your reflexes.'"
                *   **Keyframe Caption/Object Detection:** "Visual evidence confirms correct examination of the patient's neck. A keyframe at 30.2s shows the student palpating the neck, and object detection at the same time confirms their hand was on the neck."
                *   **Temporal Segment:** "The temporal action segmenter identified 'handwashing' occurring between 5.0s and 15.0s."
                *   **Pose Analyzer:** "Analysis of keyframes, such as one at 45.1s, indicates the student maintained an appropriate professional posture, appearing upright."
                *   **Scene Interaction:** "The student correctly used the tuning fork, as shown by scene interaction analysis at 50.5s depicting 'student strikes tuning_fork'."
                *   **Missing/Weak Evidence:** "Evidence for empathetic communication was weak; audio transcripts did not capture phrases of reassurance around the time of the difficult news." or "Visual confirmation of equipment X was not possible because the object_detector tool reported an error."
            *   Keep the rationale concise and focused on how the evidence supports the grade for the *specific rubric question*.

        Rubric Question:
        "{rubric_question}"

        Evidence from Automated Analysis Tools:
        --- BEGIN EVIDENCE ---
        {formatted_evidence}
        --- END EVIDENCE ---

        Scoring Criteria:
        {self.SCORING_CRITERIA}

        Output Format Instructions:
        You MUST return your response as a single, valid JSON object.
        The JSON object must have exactly two keys:
        1.  `grade`: An integer value between 0 and 5 (inclusive).
        2.  `rationale`: A string providing a concise justification for the grade, based on the evidence and referencing it.

        Example of a valid JSON output:
        {{
        "grade": 3,
        "rationale": "Satisfactory performance overall. The student was observed using a stethoscope on the patient's chest around 25.3s, as indicated by object detection. However, the subsequent explanation of findings, heard in the audio transcript from 30.1s to 32.5s, lacked sufficient detail. The student's posture at 26.0s appeared appropriate based on pose analysis."
        }}

        You MUST return only the JSON object. Do not return any additional explanation or text other than the JSON output.
        """
        return prompt 
    
    def run(
        self, 
        rubric_question: str, 
        evidence_from_executor: Dict[str, List[Dict[str, Any]]]
    ):
        if not evidence_from_executor:
            print("Scorer: No evidence provided from Executor (empty dictionary). Cannot score.")
            try: return ScorerOutput(grade=0, rationale="No evidence was provided by the automated tools to assess performance on this rubric question.")
            except Exception as e: print(f"Error creating default ScorerOutput: {e}"); return None
        
        all_tools_empty = True
        for tool_name, items in evidence_from_executor.items():
            if isinstance(items, list) and len(items) > 0: 
                all_tools_empty = False
                break

        if all_tools_empty:
            print("Scorer: All tool outputs are empty. Assuming no evidence found.")
            try: 
                output = ScorerOutput(grade=0, rationale="Automated tools ran but found no specific evidence items relevant to this rubric question.")
                return output.model_dump()
            
            except Exception as e: 
                print(f"Error creating default ScorerOutput for empty tools: {e}")
                return None

        formatted_evidence = self._format_evidence_for_prompt(evidence_from_executor)
        prompt = self._construct_prompt(rubric_question, formatted_evidence)

        try:
            response_str = generate_text_content_gemini_with_retry(
                client=self.gemini_client, 
                prompt_text=prompt
            )

            print(response_str)

            if response_str.strip().startswith("```json"): response_str = response_str.strip()[7:-3].strip()
            elif response_str.strip().startswith("```"): response_str = response_str.strip()[3:-3].strip()
            
            response_json = json.loads(response_str)
            scorer_output = ScorerOutput(**response_json)

            scorer_output_dict = scorer_output.model_dump()
            return scorer_output_dict 
        
        except json.JSONDecodeError:
            print(f"Error: Scorer LLM did not return valid JSON. Response: >>>{response_str}<<<")
            return None
        
        except Exception as e:
            print(f"An error occurred during Scorer LLM call or parsing/validation: {e}")
            traceback.print_exc()
            return None